# inference_graph.py (UPGRADED with CoT + Self-Refinement)

from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from logic_utils import LogicSolver, check_logic_validity
from dataclasses import dataclass
from typing import Optional, List

# Define the "state" that will move through the graph
@dataclass
class InferenceState:
    question: str                        # Original user question
    relevant_facts: Optional[str] = None # Knowledge base facts (Prolog text)
    logic_rule: Optional[str] = None     # Logic rules generated by LLM
    final_solution: Optional[List] = None # The final reasoning result
    retry_count: int = 0                 # Tracks how many times refinement was tried
    errors: Optional[List[str]] = None   # Stores syntax errors from logic checking
    _next: Optional[str] = None          # Holds decision about which node to run next

# First node: Load the knowledge base from file
def load_kb_node(state):
    with open("kb.txt", "r") as f:       # Open kb.txt file
        kb_facts = f.read()              # Read the whole content as a string
    state.relevant_facts = kb_facts      # Save into state
    return state                         # Pass the updated state forward

# Node: Generate logic rules from facts + description using LLM
def logic_generate_node(state):
    # Build a reusable prompt template
    prompt = PromptTemplate(
        input_variables=["context", "description"],
        template=(
            "You are given some background knowledge in Prolog format:\n\n{context}\n\n"
            "Translate the following description into Prolog-style logic rules."
            " Only output logic rules. No explanations, no extra text."
            " Each rule must end with a period.\n\n"
            "{description}"
        )
    )
    # Use OpenAI LLM (GPT-3.5-turbo here)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    # Fill in the template with knowledge base and description
    full_prompt = prompt.format(
        context=state.relevant_facts,
        description="""Define the following relations:
- Uncle: uncle(X, Y) :- parent(Z, Y), sibling(X, Z).
- Aunt: aunt(X, Y) :- parent(Z, Y), sibling(X, Z).
- Cousin: cousin(X, Y) :- parent(Z, X), parent(W, Y), sibling(Z, W).
- Grandparent: grandparent(X, Y) :- parent(X, Z), parent(Z, Y).
- Great-grandparent: greatgrandparent(X, Y) :- parent(X, Z), parent(Z, W), parent(W, Y)."""
    )

    # Call the model with the prompt
    response = llm.invoke(full_prompt)
    # Save the generated rules into state
    state.logic_rule = response.content
    return state

# Node: Check if the generated rules are valid syntax
def check_validity_node(state):
    errors = check_logic_validity(state.logic_rule)  # Run your custom syntax checker
    if not errors:
        state._next = "Solve"        # If no errors, go straight to solving
    else:
        state.errors = errors        # If errors exist, save them
        state._next = "SelfRefine"   # Decide to try refinement
    return state

# Node: Self-refine the logic rules if syntax errors are found
def self_refine_node(state):
    if state.retry_count >= 3:              # Stop if refinement attempted too many times
        print("Maximum refinement attempts reached.")
        return state

    # New LLM instance for refinement
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    # Prompt tells the LLM to ONLY fix syntax errors
    prompt = PromptTemplate(
        input_variables=["broken_logic", "errors"],
        template=(
            "The following Prolog logic has syntax errors:\n\n{errors}\n\n"
            "Please fix the syntax errors while keeping all the original facts and rules.\n"
            "Do NOT add, remove, or answer anything.\n\n"
            "{broken_logic}"
        )
    )
    # Fill in prompt with bad logic and error messages
    full_prompt = prompt.format(
        broken_logic=state.logic_rule,
        errors="\n".join(state.errors)
    )

    # Call model to fix the logic
    response = llm.invoke(full_prompt)
    # Update logic in state with corrected version
    state.logic_rule = response.content
    # Increment retry count
    state.retry_count += 1
    return state

# Node: Actually solve the problem using the LogicSolver
def solve_node(state):
    logic_solver = LogicSolver()  # Create instance of your solver
    # Combine KB facts with the generated logic rules
    full_logic = state.relevant_facts + "\n" + state.logic_rule
    # Solve and return result
    solution = logic_solver.solve_logic(full_logic)
    state.final_solution = solution
    return state

# Build the actual state graph
graph = StateGraph(InferenceState)       # Graph will move InferenceState objects
graph.add_node("LoadKB", load_kb_node)   # Register node functions
graph.add_node("GenerateLogic", logic_generate_node)
graph.add_node("CheckValidity", check_validity_node)
graph.add_node("SelfRefine", self_refine_node)
graph.add_node("Solve", solve_node)

# Define edges between nodes (execution order)
graph.set_entry_point("LoadKB")                # First node to run
graph.add_edge("LoadKB", "GenerateLogic")      # After KB -> generate logic
graph.add_edge("GenerateLogic", "CheckValidity") # Then check validity
# Conditional: from CheckValidity go to Solve or SelfRefine based on _next
graph.add_conditional_edges(
    "CheckValidity",
    lambda state: state._next
)
# After self-refine, go back to check validity again
graph.add_edge("SelfRefine", "CheckValidity")

# Compile graph into runnable workflow
compiled_graph = graph.compile()
